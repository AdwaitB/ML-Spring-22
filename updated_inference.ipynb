{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kKxuRFX9lM7k"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import glob\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.quantization import DeQuantStub, QuantStub\n",
        "from torchvision.models import alexnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My\\ Drive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971R8HkEwyED",
        "outputId": "a4cba097-6389-46a9-818b-1ce4b0b0a903"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class ImageLoader(data.Dataset):\n",
        "  '''\n",
        "  Class for data loading\n",
        "  '''\n",
        "\n",
        "  train_folder = 'COVID' # class attributes: train and test folder names\n",
        "  test_folder = 'COVID'\n",
        "\n",
        "  def __init__(self,\n",
        "               root_dir: str,\n",
        "               split: str = 'train',\n",
        "               transform: torchvision.transforms.Compose = None):\n",
        "    '''\n",
        "    Init function for the class.\n",
        "\n",
        "    Args:\n",
        "    - root_dir: the dir path which contains the train and test folder\n",
        "    - split: 'test' or 'train' split\n",
        "    - transforms: the transforms to be applied to the data\n",
        "    '''\n",
        "    #self.root = os.path.expanduser(root_dir)\n",
        "    self.transform = transform\n",
        "    self.split = split\n",
        "\n",
        "    if split == 'train':\n",
        "      self.curr_folder = os.path.join(self.train_folder)\n",
        "    elif split == 'test':\n",
        "      self.curr_folder = os.path.join(self.test_folder)\n",
        "\n",
        "    self.class_dict = self.get_classes()\n",
        "    self.dataset = self.load_imagepaths_with_labels(self.class_dict)\n",
        "    random.shuffle(self.dataset)\n",
        "    # change here\n",
        "    self.train_dataset = self.dataset[:2000]\n",
        "    self.test_dataset = self.dataset[2000:2200]\n",
        "    if split == 'train':\n",
        "      self.dataset = self.train_dataset\n",
        "    elif split == 'test':\n",
        "      self.dataset = self.test_dataset\n",
        "\n",
        "    print(f\"Train {len(self.train_dataset)}\")\n",
        "    print(f\"Test {len(self.test_dataset)}\")\n",
        "    print(f\"Full {len(self.dataset)}\")\n",
        "    \n",
        "  def load_imagepaths_with_labels(self,\n",
        "                                  class_labels: Dict[str, int]\n",
        "                                  ) -> List[Tuple[str, int]]:\n",
        "    '''\n",
        "    Fetches all image paths along with labels\n",
        "\n",
        "    Args:\n",
        "    -   class_labels: the class labels dictionary, with keys being the classes\n",
        "                      in this dataset and the values being the class index.\n",
        "    Returns:\n",
        "    -   list[(filepath, int)]: a list of filepaths and their class indices\n",
        "    '''\n",
        "    img_paths = []  # a list of (filename, class index)\n",
        "    for class_name, class_idx in class_labels.items():\n",
        "      img_dir = os.path.join(self.curr_folder, class_name, 'images/*.png')\n",
        "      files = glob.glob(img_dir)\n",
        "      img_paths += [(f, class_idx) for f in files]\n",
        "    return img_paths\n",
        "\n",
        "  def get_classes(self) -> Dict[str, int]:\n",
        "    '''\n",
        "    Get the classes (which are folder names in self.curr_folder) along with\n",
        "    their associated integer index.\n",
        "\n",
        "    Returns:\n",
        "    -   Dict of class names (string) to integer labels\n",
        "    '''\n",
        "\n",
        "    classes = dict()\n",
        "    classes_list = [d.name for d in os.scandir(self.curr_folder) if d.is_dir()]\n",
        "    classes = {classes_list[i]: i for i in range(len(classes_list))}\n",
        "    return classes\n",
        "\n",
        "  def load_img_from_path(self, path: str) -> Image:\n",
        "    ''' \n",
        "    Loads the image as grayscale (using Pillow)\n",
        "\n",
        "    Args:\n",
        "    -   path: the path of the image\n",
        "    Returns:\n",
        "    -   image: grayscale image loaded using pillow (Use 'L' flag while converting using Pillow's function)\n",
        "    '''\n",
        "\n",
        "    img = None\n",
        "    with open(path, 'rb') as f:\n",
        "      img = Image.open(f)\n",
        "      img = img.convert('L')\n",
        "    return img\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.tensor, int]:\n",
        "    '''\n",
        "    Fetches the item (image, label) at a given index\n",
        "\n",
        "    Args:\n",
        "        index (int): Index\n",
        "    Returns:\n",
        "        tuple: (image, target) where target is index of the target class.\n",
        "    '''\n",
        "    img = None\n",
        "    class_idx = None\n",
        "    filename, class_idx = self.dataset[index]\n",
        "    # load the image and apply the transforms\n",
        "    img = self.load_img_from_path(filename)\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img, class_idx\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the number of items in the dataset\n",
        "\n",
        "    Returns:\n",
        "        int: length of the dataset\n",
        "    \"\"\"\n",
        "    l = len(self.dataset)\n",
        "    return l"
      ],
      "metadata": {
        "id": "QXxyopjSwebH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_loader = ImageLoader(\"./\", split='train',)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3bnaTf4O8w8",
        "outputId": "77458b99-7620-4603-dc07-6cd7777ee88a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 2000\n",
            "Test 200\n",
            "Full 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset: \", len(image_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss0lxIApSOaD",
        "outputId": "d0374ad5-5696-4aa0-bf97-eb1039347957"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset:  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classes: \", image_loader.get_classes())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnfqHA3TSaSe",
        "outputId": "9f1afe58-3ab3-478a-ba7c-4c08fb948dec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  {'Lung_Opacity': 0, 'Viral Pneumonia': 1, 'Normal': 2, 'COVID': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetwork(nn.Module):\n",
        "  def __init__(self,aux_logits = False):\n",
        "      super(AlexNetwork, self).__init__()\n",
        "      self.cnn = nn.Sequential(\n",
        "        nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.LocalResponseNorm(96),\n",
        "        \n",
        "        nn.Conv2d(96, 384, kernel_size=5, stride = 2,padding = 2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.LocalResponseNorm(384),\n",
        "        \n",
        "        nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(384),\n",
        "        \n",
        "        nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(384),\n",
        "        \n",
        "        nn.Conv2d(384, 256, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2,padding = 1),\n",
        "      )\n",
        "      self.fc6 = nn.Sequential(\n",
        "        nn.Linear(256,4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm1d(4096),\n",
        "      )\n",
        "      self.fc = nn.Sequential(\n",
        "        nn.Linear(2*4096,4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Linear(4096, 8)\n",
        "      )\n",
        "\n",
        "  def forward_once(self, x):\n",
        "    output= self.cnn(x)\n",
        "    output = output.view(output.size()[0], -1)\n",
        "    output = self.fc6(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "  def forward(self, uniform_patch, random_patch):\n",
        "    output_fc6_uniform = self.forward_once(uniform_patch)\n",
        "    output_fc6_random = self.forward_once(random_patch)\n",
        "    output = torch.cat((output_fc6_uniform,output_fc6_random), 1)\n",
        "    output = self.fc(output)\n",
        "    return output, output_fc6_uniform, output_fc6_random"
      ],
      "metadata": {
        "id": "F5ln6AfGywIp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"alexnet_kaggle_context_pred.pt\")\n"
      ],
      "metadata": {
        "id": "qRW4z8AiO1cT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetwork(nn.Module):\n",
        "  def __init__(self,aux_logits = False):\n",
        "      super(AlexNetwork, self).__init__()\n",
        "      self.cnn = nn.Sequential(\n",
        "        nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.LocalResponseNorm(96),\n",
        "        \n",
        "        nn.Conv2d(96, 384, kernel_size=5, stride = 2,padding = 2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.LocalResponseNorm(384),\n",
        "        \n",
        "        nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(384),\n",
        "        \n",
        "        nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(384),\n",
        "        \n",
        "        nn.Conv2d(384, 256, kernel_size=3, stride=1,padding = 1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2,padding = 1),\n",
        "      )\n",
        "      self.fc6 = nn.Sequential(\n",
        "        nn.Linear(256,4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm1d(4096),\n",
        "      )\n",
        "      self.fc = nn.Sequential(\n",
        "        nn.Linear(2*4096,4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Linear(4096, 8)\n",
        "      )\n",
        "\n",
        "  def forward_once(self, x):\n",
        "    output= self.cnn(x)\n",
        "    output = output.view(output.size()[0], -1)\n",
        "    output = self.fc6(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "  def forward(self, uniform_patch, random_patch):\n",
        "    output_fc6_uniform = self.forward_once(uniform_patch)\n",
        "    output_fc6_random = self.forward_once(random_patch)\n",
        "    output = torch.cat((output_fc6_uniform,output_fc6_random), 1)\n",
        "    output = self.fc(output)\n",
        "    return output, output_fc6_uniform, output_fc6_random"
      ],
      "metadata": {
        "id": "bUBlfk_IUg0r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_alexnet = AlexNetwork()"
      ],
      "metadata": {
        "id": "fkH3vjLcZE2I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.keys())\n",
        "print(model[\"model_state_dict\"].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP83xG5xZhO8",
        "outputId": "1c3ebe30-1a20-43c2-90f5-1842e2edabb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'global_trnloss', 'global_valloss'])\n",
            "odict_keys(['cnn.0.weight', 'cnn.0.bias', 'cnn.4.weight', 'cnn.4.bias', 'cnn.8.weight', 'cnn.8.bias', 'cnn.10.weight', 'cnn.10.bias', 'cnn.10.running_mean', 'cnn.10.running_var', 'cnn.10.num_batches_tracked', 'cnn.11.weight', 'cnn.11.bias', 'cnn.13.weight', 'cnn.13.bias', 'cnn.13.running_mean', 'cnn.13.running_var', 'cnn.13.num_batches_tracked', 'cnn.14.weight', 'cnn.14.bias', 'cnn.16.weight', 'cnn.16.bias', 'cnn.16.running_mean', 'cnn.16.running_var', 'cnn.16.num_batches_tracked', 'fc6.0.weight', 'fc6.0.bias', 'fc6.2.weight', 'fc6.2.bias', 'fc6.2.running_mean', 'fc6.2.running_var', 'fc6.2.num_batches_tracked', 'fc.0.weight', 'fc.0.bias', 'fc.2.weight', 'fc.2.bias', 'fc.4.weight', 'fc.4.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_alexnet.load_state_dict(model[\"model_state_dict\"], strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WV2NidqZIXo",
        "outputId": "4d460f23-7b7b-4c27-95aa-73c86e60f340"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_alexnet.cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNAdq77ZZZbb",
        "outputId": "f7b0e6e8-0b79-40a5-c26b-2d5c46555367"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): LocalResponseNorm(96, alpha=0.0001, beta=0.75, k=1.0)\n",
            "  (4): Conv2d(96, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): LocalResponseNorm(384, alpha=0.0001, beta=0.75, k=1.0)\n",
            "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (12): ReLU(inplace=True)\n",
            "  (13): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (17): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNclassifier(nn.Module):\n",
        "  def __init__(self, pretrained_cnn):\n",
        "      super(CNNclassifier, self).__init__()\n",
        "      self.loss_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "      self.cnn = pretrained_cnn\n",
        "      self.fc = nn.Sequential(\n",
        "        nn.Linear(4096,4),)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    new_x = x.repeat(1, 3, 1, 1)\n",
        "    output= self.cnn(new_x)\n",
        "    output = output.view(output.size()[0], -1)\n",
        "    output = self.fc(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "p0s_eabVfZwu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNNclassifier(my_alexnet.cnn)"
      ],
      "metadata": {
        "id": "c4sUgW_7iOrq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.optim import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "optimizer_config = {\n",
        "  \"optimizer_type\": \"adam\",\n",
        "  \"lr\": 0.0001,\n",
        "  \"weight_decay\": 1,\n",
        "}\n",
        "\n",
        "def get_optimizer(model: torch.nn.Module,\n",
        "                  config: dict) -> torch.optim.Optimizer:\n",
        "  '''\n",
        "  Returns the optimizer for the model params, initialized according to the config.\n",
        "\n",
        "  Note: config has a minimum of three entries. Feel free to add more entries if you want.\n",
        "  But do not change the name of the three existing entries. \n",
        "  Of course, the optimizer should be operating on the model.\n",
        "\n",
        "\n",
        "  Args:\n",
        "  - model: the model to optimize for\n",
        "  - config: a dictionary containing parameters for the config\n",
        "  Returns:\n",
        "  - optimizer: the optimizer\n",
        "  '''\n",
        "\n",
        "  optimizer = None\n",
        "\n",
        "  optimizer_type = config[\"optimizer_type\"]\n",
        "  learning_rate = config[\"lr\"]\n",
        "  weight_decay = config[\"weight_decay\"]\n",
        "\n",
        "  ############################################################################\n",
        "  # Student code begin\n",
        "  ############################################################################\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "  ############################################################################\n",
        "  # Student code end\n",
        "  ############################################################################\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "def compute_loss(model: torch.nn.Module,\n",
        "                 model_output: torch.tensor,\n",
        "                 target_labels: torch.tensor,\n",
        "                 is_normalize: bool = True) -> torch.tensor:\n",
        "  '''\n",
        "  Computes the loss between the model output and the target labels\n",
        "\n",
        "  Note: we have initialized the loss_criterion in the model with the sum\n",
        "  reduction.\n",
        "\n",
        "  Args:\n",
        "  -   model: model (which inherits from nn.Module), and contains loss_criterion\n",
        "  -   model_output: the raw scores output by the net [Dim: (N, 15)]\n",
        "  -   target_labels: the ground truth class labels [Dim: (N, )]\n",
        "  -   is_normalize: bool flag indicating that loss should be divided by the\n",
        "                    batch size\n",
        "  Returns:\n",
        "  -   the loss value of the input model\n",
        "  '''\n",
        "\n",
        "  ############################################################################\n",
        "  # Student code begin\n",
        "  ############################################################################\n",
        "  # obtained_labels = predict_labels(model_output)\n",
        "  # obtained_labels = nn.functional.softmax(obtained_labels, dim=1)\n",
        "  kld = model.loss_criterion\n",
        "  loss = kld(model_output,target_labels)\n",
        "  if is_normalize:\n",
        "    loss = loss/len(target_labels)\n",
        "\n",
        "  ############################################################################\n",
        "  # Student code end\n",
        "  ############################################################################\n",
        "\n",
        "  return loss\n",
        "\n",
        "def predict_labels(model_output: torch.tensor) -> torch.tensor:\n",
        "  '''\n",
        "  Predicts the labels from the output of the model.\n",
        "\n",
        "  Args:\n",
        "  -   model_output: the model output [Dim: (N, 15)]\n",
        "  Returns:\n",
        "  -   predicted_labels: the output labels [Dim: (N,)]\n",
        "  '''\n",
        "\n",
        "  ############################################################################\n",
        "  # Student code begin\n",
        "  ############################################################################\n",
        "  _, predicted_labels = torch.max(model_output,dim=1)\n",
        "\n",
        "    \n",
        "  ############################################################################\n",
        "  # Student code end\n",
        "  ############################################################################\n",
        "\n",
        "  return predicted_labels\n",
        "\n",
        "\n",
        "optimizer = get_optimizer(cnn_model, optimizer_config)\n",
        "\n",
        "input_size = (64,64)\n",
        "\n",
        "def get_fundamental_transforms() -> transforms.Compose:\n",
        "  '''\n",
        "  Returns the core transforms needed to feed the images to our model\n",
        "\n",
        "  Suggestions: Resize(), ToTensor(), Normalize() from torchvision.transforms\n",
        "\n",
        "  Args:\n",
        "  - inp_size: tuple denoting the dimensions for input to the model\n",
        "  - pixel_mean: the mean of the raw dataset [Shape=(1,)]\n",
        "  - pixel_std: the standard deviation of the raw dataset [Shape=(1,)]\n",
        "  Returns:\n",
        "  - fundamental_transforms: transforms.Compose with the fundamental transforms\n",
        "  '''\n",
        "\n",
        "  return transforms.Compose([\n",
        "      ############################################################################\n",
        "      # Student code begin\n",
        "      ############################################################################\n",
        "    #transforms.Resize(size=inp_size),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=pixel_mean,std=pixel_std)\n",
        "\n",
        "      ############################################################################\n",
        "      # Student code end\n",
        "      ############################################################################\n",
        "  ])\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "  '''\n",
        "  This class makes training the model easier\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               data_dir,\n",
        "               model,\n",
        "               optimizer,\n",
        "               model_dir,\n",
        "               train_data_transforms,\n",
        "               test_data_transforms,\n",
        "               batch_size=100,\n",
        "               load_from_disk=True,\n",
        "               cuda=False\n",
        "               ):\n",
        "    self.model_dir = model_dir\n",
        "\n",
        "    self.model = model\n",
        "\n",
        "    self.cuda = cuda\n",
        "    if cuda:\n",
        "      self.model.cuda()\n",
        "\n",
        "    dataloader_args = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "    self.train_dataset = ImageLoader(\n",
        "        data_dir, split='train', transform=train_data_transforms)\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_dataset, \n",
        "                                                    batch_size=batch_size, \n",
        "                                                    shuffle=True,\n",
        "                                                    **dataloader_args)\n",
        "\n",
        "    self.test_dataset = ImageLoader(\n",
        "        data_dir, split='test', transform=test_data_transforms)\n",
        "    self.test_loader = torch.utils.data.DataLoader(self.test_dataset, \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True,\n",
        "                                                   **dataloader_args)\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.train_loss_history = []\n",
        "    self.validation_loss_history = []\n",
        "    self.train_accuracy_history = []\n",
        "    self.validation_accuracy_history = []\n",
        "\n",
        "    # load the model from the disk if it exists\n",
        "    if os.path.exists(model_dir) and load_from_disk:\n",
        "      checkpoint = torch.load(os.path.join(self.model_dir, 'checkpoint.pt'))\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "  def save_model(self):\n",
        "    '''\n",
        "    Saves the model state and optimizer state on the dict\n",
        "    '''\n",
        "    torch.save({\n",
        "        'model_state_dict': self.model.state_dict(),\n",
        "        'optimizer_state_dict': self.optimizer.state_dict()\n",
        "    }, os.path.join(self.model_dir, 'checkpoint.pt'))\n",
        "\n",
        "  def train(self, num_epochs):\n",
        "    '''\n",
        "    The main train loop\n",
        "    '''\n",
        "    self.model.train()\n",
        "    #print(\"test\")\n",
        "\n",
        "    train_loss, train_acc = self.evaluate(split='train')\n",
        "    #print(\"test\")\n",
        "    val_loss, val_acc = self.evaluate(split='test')\n",
        "    #print(\"test\")\n",
        "\n",
        "    self.train_loss_history.append(train_loss)\n",
        "    self.train_accuracy_history.append(train_acc)\n",
        "    self.validation_loss_history.append(val_loss)\n",
        "    self.validation_accuracy_history.append(val_acc)\n",
        "\n",
        "    print('Epoch:{}, Training Loss:{:.4f}, Validation Loss:{:.4f}'.format(\n",
        "      0, self.train_loss_history[-1], self.validation_loss_history[-1])\n",
        "    )\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "      self.model.train()\n",
        "      for _, batch in enumerate(self.train_loader):\n",
        "        if self.cuda:\n",
        "          input_data, target_data = Variable(\n",
        "              batch[0]).cuda(), Variable(batch[1]).cuda()\n",
        "        else:\n",
        "          input_data, target_data = Variable(batch[0]), Variable(batch[1])\n",
        "\n",
        "        #print(input_data.shape)\n",
        "        #print(target_data.shape)\n",
        "\n",
        "        output_data = self.model(input_data)\n",
        "        loss = compute_loss(self.model, output_data, target_data)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      train_loss, train_acc = self.evaluate(split='train')\n",
        "      val_loss, val_acc = self.evaluate(split='test')\n",
        "\n",
        "      self.train_loss_history.append(train_loss)\n",
        "      self.train_accuracy_history.append(train_acc)\n",
        "      self.validation_loss_history.append(val_loss)\n",
        "      self.validation_accuracy_history.append(val_acc)\n",
        "\n",
        "      print('Epoch:{}, Training Loss:{:.4f}, Validation Loss:{:.4f}'.format(\n",
        "        epoch_idx+1, self.train_loss_history[-1], self.validation_loss_history[-1])\n",
        "      )\n",
        "\n",
        "    self.save_model()\n",
        "\n",
        "\n",
        "  def evaluate(self, split='test'):\n",
        "    '''\n",
        "    Get the loss and accuracy on the test/train dataset\n",
        "    '''\n",
        "    self.model.eval()\n",
        "\n",
        "    num_examples = 0\n",
        "    num_correct = 0\n",
        "    loss = 0\n",
        "\n",
        "    for _, batch in enumerate(self.test_loader if split == 'test' else self.train_loader):\n",
        "      #print(\"here\")\n",
        "      if self.cuda:\n",
        "        input_data, target_data = Variable(\n",
        "            batch[0]).cuda(), Variable(batch[1]).cuda()\n",
        "      else:\n",
        "        input_data, target_data = Variable(batch[0]), Variable(batch[1])\n",
        "\n",
        "      output_data = self.model(input_data)\n",
        "\n",
        "      num_examples += input_data.shape[0]\n",
        "      loss += float(compute_loss(self.model,\n",
        "                          output_data, target_data, is_normalize=False))\n",
        "      predicted_labels = predict_labels(output_data)\n",
        "      num_correct += torch.sum(predicted_labels == target_data).cpu().item()\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "    return loss/float(num_examples), float(num_correct)/float(num_examples)\n",
        "\n",
        "  def plot_loss_history(self):\n",
        "    '''\n",
        "    Plots the loss history\n",
        "    '''\n",
        "    plt.figure()\n",
        "    ep = range(len(self.train_loss_history))\n",
        "\n",
        "    plt.plot(ep, self.train_loss_history, '-b', label = 'training')\n",
        "    plt.plot(ep, self.validation_loss_history, '-r', label = 'validation')\n",
        "    plt.title(\"Loss history\")\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.show()\n",
        "\n",
        "  def plot_accuracy(self):\n",
        "    '''\n",
        "    Plots the accuracy history\n",
        "    '''\n",
        "    plt.figure()\n",
        "    ep = range(len(self.train_accuracy_history))\n",
        "    plt.plot(ep, self.train_accuracy_history, '-b', label = 'training')\n",
        "    plt.plot(ep, self.validation_accuracy_history, '-r', label = 'validation')\n",
        "    plt.title(\"Accuracy history\")\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "trainer = Trainer(data_dir=\"./\", \n",
        "                  model = cnn_model,\n",
        "                  optimizer = optimizer,\n",
        "                  model_dir = os.path.join(\"./\", 'cnn_net'),\n",
        "                  train_data_transforms = get_fundamental_transforms(),\n",
        "                  test_data_transforms = get_fundamental_transforms(),\n",
        "                  batch_size = 32,\n",
        "                  load_from_disk = False,\n",
        "                  cuda = True\n",
        "                 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaRFrxq-iVQT",
        "outputId": "4ae2c6d8-0156-45d3-a61e-819b40ea8c46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 2000\n",
            "Test 200\n",
            "Full 2000\n",
            "Train 2000\n",
            "Test 200\n",
            "Full 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "uuM1a1PJjEj-",
        "outputId": "882879d4-b83a-455d-d4fd-5106eb1993ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0, Training Loss:2460.0835, Validation Loss:769.0714\n",
            "Epoch:1, Training Loss:1.1684, Validation Loss:1.2310\n",
            "Epoch:2, Training Loss:1.1076, Validation Loss:1.1768\n",
            "Epoch:3, Training Loss:1.0857, Validation Loss:1.1373\n",
            "Epoch:4, Training Loss:1.0567, Validation Loss:1.0923\n",
            "Epoch:5, Training Loss:1.0512, Validation Loss:1.0839\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-69cdd2a1a0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-a77540373571>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    252\u001b[0m       )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-a77540373571>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     }, os.path.join(self.model_dir, 'checkpoint.pt'))\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cnn_net/checkpoint.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = trainer.train_accuracy_history[-1]\n",
        "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
        "\n",
        "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HMYHNN9pzE3",
        "outputId": "9938c8b1-d238-448c-9956-85a10a39ebbc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy = 0.5835; Validation Accuracy = 0.5400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8-H6raaP2XLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}